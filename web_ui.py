import streamlit as st
import time
from load_model import load_llm
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import json
from pathlib import Path
import glob

TOP_K = 3
INDEX_DIR = Path("data/index")
EMBEDDING_PATH = "data/encoder/bge-m3"

@st.cache_resource
def load_embedding_model():
    return SentenceTransformer(EMBEDDING_PATH, device='cpu')

@st.cache_resource
def load_all_indexes():
    all_indexes = []
    all_mappings = []
    for faiss_path in glob.glob(str(INDEX_DIR / "**/index.faiss"), recursive=True):
        try:
            index = faiss.read_index(faiss_path)
            mapping_file = Path(faiss_path).parent / "mapping.json"
            if not mapping_file.exists():
                continue
            with open(mapping_file, "r", encoding="utf-8") as f:
                mapping = json.load(f)
            all_indexes.append(index)
            all_mappings.append(mapping)
        except Exception as e:
            st.error(f"L·ªói khi load {faiss_path}: {e}")
    return all_indexes, all_mappings

def search_similar_chunks(query, model, all_indexes, all_mappings, top_k=TOP_K):
    query_emb = model.encode(
        f"Represent this sentence for searching relevant passages: {query}",
        convert_to_numpy=True
    )
    results = []
    for index, texts in zip(all_indexes, all_mappings):
        D, I = index.search(np.array([query_emb]), top_k)
        for score, idx in zip(D[0], I[0]):
            if 0 <= idx < len(texts):
                results.append((score, texts[idx]))
    results = sorted(results, key=lambda x: x[0])[:top_k]
    return [r[1] for r in results]

def limit_context(chunks, max_chars=800):
    context = ""
    for c in chunks:
        if len(context) + len(c) > max_chars:
            break
        context += c + "\n\n"
    return context.strip()

# ========= STREAMLIT APP ========= #
st.title("ü§ñ IDCee - RAG")

query = st.text_input("Nh·∫≠p c√¢u h·ªèi:")

if "llm" not in st.session_state:
    with st.spinner("üîÅ ƒêang kh·ªüi ƒë·ªông m√¥ h√¨nh..."):
        st.session_state.llm = load_llm()
        st.session_state.embedding = load_embedding_model()
        st.session_state.indexes, st.session_state.mappings = load_all_indexes()
        st.success("‚úÖ S·∫µn s√†ng!")

if query:
    with st.spinner("üß† ƒêang t√¨m c√¢u tr·∫£ l·ªùi..."):
        start = time.time()
        chunks = search_similar_chunks(
            query,
            st.session_state.embedding,
            st.session_state.indexes,
            st.session_state.mappings
        )
        context = limit_context(chunks)

        prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI IDCee. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, b·∫±ng ti·∫øng Vi·ªát v√† ch·ªâ d·ª±a v√†o ph·∫ßn Th√¥ng tin n·ªôi b·ªô b√™n d∆∞·ªõi.

‚ùó Kh√¥ng ƒë∆∞·ª£c suy ƒëo√°n, kh√¥ng ƒë∆∞·ª£c b·ªãa.
‚ùó N·∫øu trong vƒÉn b·∫£n c√≥ ghi c·ª• th·ªÉ (s·ªë li·ªáu, th·ªùi gian, ng∆∞·ªùi ch·ªãu tr√°ch nhi·ªám, ƒë·ªãnh k·ª≥...), ph·∫£i tr·∫£ l·ªùi ch√≠nh x√°c kh√¥ng thi·∫øu s√≥t.
‚ùó N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan, ch·ªâ ƒë∆∞·ª£c tr·∫£ l·ªùi: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu n·ªôi b·ªô ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

C√¢u h·ªèi: {query}

Th√¥ng tin n·ªôi b·ªô:
{context}

Tr·∫£ l·ªùi:
"""

        try:
            response = ""
            display = st.empty()

            result = st.session_state.llm(prompt, max_tokens=192, stream=True)

            # ‚úÖ X·ª≠ l√Ω ƒë√∫ng ƒë·ªãnh d·∫°ng dict t·ª´ stream
            if hasattr(result, "__iter__") and not isinstance(result, dict):
                for chunk in result:
                    if isinstance(chunk, dict) and "choices" in chunk:
                        delta = chunk["choices"][0]["text"]
                    else:
                        delta = str(chunk)
                    response += delta
                    display.markdown(f"#### ü§ñ IDCee tr·∫£ l·ªùi:\n\n{response.strip() + '‚ñå'}")
                display.markdown(f"#### ü§ñ IDCee tr·∫£ l·ªùi:\n\n{response.strip()}")

            elif isinstance(result, str):
                display.markdown(f"#### ü§ñ IDCee tr·∫£ l·ªùi:\n\n{result.strip()}")

            elif isinstance(result, dict) and "choices" in result:
                response = result["choices"][0]["text"].strip()
                display.markdown(f"#### ü§ñ IDCee tr·∫£ l·ªùi:\n\n{response}")

            else:
                display.markdown("ü§ñ IDCee: (Kh√¥ng th·ªÉ x·ª≠ l√Ω ph·∫£n h·ªìi)")

            elapsed = time.time() - start
            st.caption(f"‚è±Ô∏è Th·ªùi gian ph·∫£n h·ªìi: {elapsed:.2f} gi√¢y")

            with st.expander("üü® ƒêo·∫°n context ƒë∆∞·ª£c s·ª≠ d·ª•ng"):
                st.code(context, language="markdown")

            with st.expander("üìÑ Top K ƒëo·∫°n truy xu·∫•t t·ª´ FAISS"):
                for i, chunk in enumerate(chunks):
                    st.markdown(f"**{i+1}.** {chunk.strip()}")

        except Exception as e:
            st.error(f"L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {e}")
