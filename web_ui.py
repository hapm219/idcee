import streamlit as st
import time
from load_model import load_llm
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import json
from pathlib import Path
import glob

TOP_K = 5
INDEX_DIR = Path("data/index")
EMBEDDING_PATH = "data/encoder/bge-m3"

@st.cache_resource
def load_embedding_model():
    return SentenceTransformer(EMBEDDING_PATH, device='cpu')

@st.cache_resource
def load_all_indexes():
    all_indexes = []
    all_mappings = []
    for faiss_path in glob.glob(str(INDEX_DIR / "**/index.faiss"), recursive=True):
        try:
            index = faiss.read_index(faiss_path)
            mapping_file = Path(faiss_path).parent / "mapping.json"
            if not mapping_file.exists():
                continue
            with open(mapping_file, "r", encoding="utf-8") as f:
                mapping = json.load(f)
            all_indexes.append(index)
            all_mappings.append(mapping)
        except Exception as e:
            st.error(f"Lá»—i khi load {faiss_path}: {e}")
    return all_indexes, all_mappings

def search_similar_chunks(query, model, all_indexes, all_mappings, top_k=TOP_K):
    query_emb = model.encode(
        f"Represent this sentence for searching relevant passages: {query}",
        convert_to_numpy=True
    )
    results = []
    for index, texts in zip(all_indexes, all_mappings):
        D, I = index.search(np.array([query_emb]), top_k)
        for score, idx in zip(D[0], I[0]):
            if 0 <= idx < len(texts):
                results.append((score, texts[idx]))
    results = sorted(results, key=lambda x: x[0])[:top_k]
    return [r[1] for r in results]

def limit_context(chunks, max_chars=1200):
    context = ""
    for c in chunks:
        if len(context) + len(c) > max_chars:
            break
        context += c + "\n\n"
    return context.strip()

# ========= STREAMLIT APP ========= #
st.title("ğŸ¤– IDCee - RAG")

query = st.text_input("Nháº­p cÃ¢u há»i:")

if "llm" not in st.session_state:
    with st.spinner("ğŸ” Äang khá»Ÿi Ä‘á»™ng mÃ´ hÃ¬nh..."):
        st.session_state.llm = load_llm()
        st.session_state.embedding = load_embedding_model()
        st.session_state.indexes, st.session_state.mappings = load_all_indexes()
        st.success("âœ… Sáºµn sÃ ng!")

if query:
    with st.spinner("ğŸ§  Äang tÃ¬m cÃ¢u tráº£ lá»i..."):
        start = time.time()
        chunks = search_similar_chunks(query, st.session_state.embedding, st.session_state.indexes, st.session_state.mappings)
        context = limit_context(chunks)

        prompt = f"""Báº¡n lÃ  trá»£ lÃ½ AI IDCee. Tráº£ lá»i ngáº¯n gá»n, báº±ng tiáº¿ng Viá»‡t vÃ  chá»‰ dá»±a vÃ o pháº§n ThÃ´ng tin ná»™i bá»™ bÃªn dÆ°á»›i.

â— KhÃ´ng Ä‘Æ°á»£c suy Ä‘oÃ¡n, khÃ´ng Ä‘Æ°á»£c bá»‹a.
â— Náº¿u trong vÄƒn báº£n cÃ³ ghi cá»¥ thá»ƒ (sá»‘ liá»‡u, thá»i gian, ngÆ°á»i chá»‹u trÃ¡ch nhiá»‡m, Ä‘á»‹nh ká»³...), pháº£i tráº£ lá»i chÃ­nh xÃ¡c khÃ´ng thiáº¿u sÃ³t.
â— Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan, chá»‰ Ä‘Æ°á»£c tráº£ lá»i: "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong tÃ i liá»‡u ná»™i bá»™ Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y."

CÃ¢u há»i: {query}

ThÃ´ng tin ná»™i bá»™:
{context}

Tráº£ lá»i:
"""

        try:
            response = ""
            with st.empty():
                for chunk in st.session_state.llm(prompt, max_new_tokens=192, stop=["###", "CÃ¢u há»i:"], stream=True):
                    response += chunk
                    st.markdown(f"#### ğŸ¤– IDCee tráº£ lá»i:\n\n{response.strip() + 'â–Œ'}")
            elapsed = time.time() - start
            st.caption(f"â±ï¸ Thá»i gian pháº£n há»“i: {elapsed:.2f} giÃ¢y")

            # ğŸ” Hiá»ƒn thá»‹ toÃ n bá»™ context Ä‘Ã£ dÃ¹ng
            with st.expander("ğŸŸ¨ Äoáº¡n context Ä‘Æ°á»£c sá»­ dá»¥ng"):
                st.code(context, language="markdown")

            # ğŸ“„ Hiá»ƒn thá»‹ tá»«ng Ä‘oáº¡n tá»« FAISS (Top K)
            with st.expander("ğŸ“„ Top K Ä‘oáº¡n truy xuáº¥t tá»« FAISS"):
                for i, chunk in enumerate(chunks):
                    st.markdown(f"**{i+1}.** {chunk.strip()}")

        except Exception as e:
            st.error(f"Lá»—i khi táº¡o cÃ¢u tráº£ lá»i: {e}")
