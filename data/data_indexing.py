import os
import faiss
import json
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

REFINED_DIR = Path("refined")
INDEX_DIR = Path("index")
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"
EMBEDDING_LOCAL_PATH = Path("encoder/bge-m3")

processed_files = 0
all_txt_files = 0
written_dirs = set()

def load_or_download_model():
    if not EMBEDDING_LOCAL_PATH.exists():
        print(f"â¬‡ï¸ Äang táº£i mÃ´ hÃ¬nh embedding: {EMBEDDING_MODEL_NAME}")
        model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        model.save(str(EMBEDDING_LOCAL_PATH))
    else:
        print(f"ğŸ“¥ Äang load mÃ´ hÃ¬nh tá»« thÆ° má»¥c cá»¥c bá»™: {EMBEDDING_LOCAL_PATH}")
        model = SentenceTransformer(str(EMBEDDING_LOCAL_PATH))
    return model

def process_text_file(path: Path):
    text = path.read_text(encoding="utf-8", errors="ignore")
    chunks = text.split("\n\n")
    return [c.strip() for c in chunks if len(c.strip()) > 20]

def build_faiss_index_for_folder(folder: Path, model: SentenceTransformer):
    global processed_files, written_dirs
    all_texts = []
    all_embeddings = []

    for txt_file in folder.glob("*.txt"):
        texts = process_text_file(txt_file)
        if not texts:
            continue

        prompt_texts = [
            f"Represent this sentence for searching relevant passages: {t}"
            for t in texts
        ]
        embeddings = model.encode(prompt_texts, show_progress_bar=False, convert_to_numpy=True)

        all_texts.extend(texts)
        all_embeddings.append(embeddings)
        processed_files += 1

    if not all_embeddings:
        print(f"âš ï¸  KhÃ´ng cÃ³ Ä‘oáº¡n vÄƒn nÃ o trong {folder}")
        return

    all_embeddings = np.vstack(all_embeddings)

    index = faiss.IndexFlatL2(all_embeddings.shape[1])
    index.add(all_embeddings)

    rel_path = folder.relative_to(REFINED_DIR)
    out_dir = INDEX_DIR / rel_path
    out_dir.mkdir(parents=True, exist_ok=True)

    faiss.write_index(index, str(out_dir / "index.faiss"))
    with open(out_dir / "mapping.json", "w", encoding="utf-8") as f:
        json.dump(all_texts, f, ensure_ascii=False, indent=2)

    written_dirs.add(str(out_dir))
    print(f"âœ… Indexed {len(all_texts)} Ä‘oáº¡n vÄƒn â†’ {out_dir}/index.faiss")

def scan_and_index_all():
    global all_txt_files
    model = load_or_download_model()

    folders = [f for f in REFINED_DIR.glob("**/") if any(f.glob("*.txt"))]
    all_txt_files = sum(len(list(f.glob("*.txt"))) for f in folders)

    for folder in folders:
        build_faiss_index_for_folder(folder, model)

    print("\nğŸ“Š Tá»•ng káº¿t indexing:")
    print(f"ğŸ” Sá»‘ file .txt tÃ¬m tháº¥y: {all_txt_files}")
    print(f"âœï¸  Sá»‘ file Ä‘Ã£ xá»­ lÃ½: {processed_files}")
    print("ğŸ“‚ Ghi FAISS index vÃ o cÃ¡c thÆ° má»¥c:")
    for d in sorted(written_dirs):
        print(f"   - {d}")

if __name__ == "__main__":
    scan_and_index_all()
