import os
import sys
import time
import json
import faiss
import numpy as np
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

REFINED_DIR = Path("refined")
INDEX_DIR = Path("index")
MODEL_DIR = Path("encoder/bge-m3")
BATCH_SIZE = 32

def load_model():
    import torch
    if not MODEL_DIR.exists() or not (MODEL_DIR / "config.json").exists():
        print("âŒ KhÃ´ng tÃ¬m tháº¥y mÃ´ hÃ¬nh táº¡i: encoder/bge-m3")
        exit(1)
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"âœ… DÃ¹ng {'GPU Mac (MPS)' if device == 'mps' else 'CPU'}")
    try:
        model = SentenceTransformer(str(MODEL_DIR), device=device)
        print("âœ… ÄÃ£ load model thÃ nh cÃ´ng tá»«: encoder/bge-m3")
        return model
    except Exception as e:
        print("âŒ Lá»—i khi load mÃ´ hÃ¬nh embedding:")
        print(e)
        exit(1)

def get_chunks(text):
    return [c.strip() for c in text.split("\n\n") if len(c.strip()) > 30]

def encode_chunks(chunks, model):
    sys.stdout.write(f"ğŸ§  Äang encode {len(chunks)} Ä‘oáº¡n...\033[K\r")
    sys.stdout.flush()
    embeddings = model.encode(
        [f"Represent this sentence for searching relevant passages: {c}" for c in chunks],
        batch_size=BATCH_SIZE,
        show_progress_bar=False,
        normalize_embeddings=False
    )
    return np.array(embeddings)

def index_folder(folder, model, bar):
    all_texts, all_embeds = [], []
    txt_files = list(folder.glob("*.txt"))
    if not txt_files:
        return 0

    for txt_file in txt_files:
        text = txt_file.read_text(encoding="utf-8", errors="ignore")
        chunks = get_chunks(text)
        if not chunks:
            bar.update(1)
            continue

        # âœ… Äáº·t log dÆ°á»›i thanh progress bar, ghi Ä‘Ã¨ má»—i láº§n
        bar.write(f"ğŸ“„ File: {txt_file.relative_to(REFINED_DIR)}")
        bar.write(f"ğŸ§  Äang encode {len(chunks)} Ä‘oáº¡n...\033[K\r")

        embeddings = encode_chunks(chunks, model)
        all_texts.extend(chunks)
        all_embeds.append(embeddings)
        bar.update(1)

    if not all_embeds:
        return 0

    vectors = np.vstack(all_embeds)
    index = faiss.IndexFlatL2(vectors.shape[1])
    index.add(vectors)

    out_dir = INDEX_DIR / folder.relative_to(REFINED_DIR)
    out_dir.mkdir(parents=True, exist_ok=True)
    faiss.write_index(index, str(out_dir / "index.faiss"))
    with open(out_dir / "mapping.json", "w", encoding="utf-8") as f:
        json.dump(all_texts, f, ensure_ascii=False, indent=2)

    bar.write(f"âœ… ÄÃ£ index {len(all_texts)} Ä‘oáº¡n â†’ {out_dir}/index.faiss")
    return len(txt_files)

def main():
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    start_all = time.time()

    folders = sorted({f.parent for f in REFINED_DIR.rglob("*.txt")})
    files = list(REFINED_DIR.rglob("*.txt"))
    print(f"ğŸ” TÃ¬m tháº¥y {len(folders)} thÆ° má»¥c | {len(files)} file cáº§n index")

    model = load_model()
    total_indexed = 0

    with tqdm(total=len(files), desc="ğŸ“¦ Indexing toÃ n bá»™", ncols=80) as bar:
        for folder in folders:
            count = index_folder(folder, model, bar)
            total_indexed += count
        bar.close()
        print()

    print(f"âœ… HoÃ n táº¥t. ÄÃ£ index {total_indexed} file.")
    print(f"ğŸ“‚ FAISS index lÆ°u táº¡i: {INDEX_DIR}/")
    print(f"ğŸ•“ Tá»•ng thá»i gian: {time.time() - start_all:.2f} giÃ¢y")

if __name__ == "__main__":
    main()
