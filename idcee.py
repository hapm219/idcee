import time
from load_model import load_llm
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import json
from pathlib import Path
import glob

TOP_K = 1
INDEX_DIR = Path("data/index")
EMBEDDING_PATH = "data/encoder/bge-m3"

def load_embedding_model():
    print("üì• ƒêang t·∫£i m√¥ h√¨nh embedding bge-m3...")
    return SentenceTransformer(EMBEDDING_PATH)

def load_all_indexes():
    all_indexes = []
    all_mappings = []
    for faiss_path in glob.glob(str(INDEX_DIR / "**/index.faiss"), recursive=True):
        try:
            index = faiss.read_index(faiss_path)
            mapping_file = Path(faiss_path).parent / "mapping.json"
            if not mapping_file.exists():
                continue
            with open(mapping_file, "r", encoding="utf-8") as f:
                mapping = json.load(f)
            all_indexes.append(index)
            all_mappings.append(mapping)
            print(f"‚úÖ Loaded index: {faiss_path}")
        except Exception as e:
            print(f"‚ùå L·ªói khi load {faiss_path}: {e}")
    return all_indexes, all_mappings

def search_similar_chunks(query, model, all_indexes, all_mappings, top_k=TOP_K):
    query_emb = model.encode(
        f"Represent this sentence for searching relevant passages: {query}",
        convert_to_numpy=True
    )

    results = []
    for index, texts in zip(all_indexes, all_mappings):
        D, I = index.search(np.array([query_emb]), top_k)
        for score, idx in zip(D[0], I[0]):
            if 0 <= idx < len(texts):
                results.append((score, texts[idx]))

    results = sorted(results, key=lambda x: x[0])[:top_k]
    return [r[1] for r in results]

def limit_context(chunks, max_chars=800):
    context = ""
    for c in chunks:
        if len(context) + len(c) > max_chars:
            break
        context += c + "\n\n"
    return context.strip()

def main():
    llm = load_llm()
    embed_model = load_embedding_model()
    all_indexes, all_mappings = load_all_indexes()

    print("ü§ñ IDCee s·∫µn s√†ng! G√µ 'exit' ƒë·ªÉ tho√°t.\n")

    while True:
        user_input = input("üßë You: ").strip()
        if user_input.lower() in ["exit", "quit"]:
            break

        retrieved_chunks = search_similar_chunks(user_input, embed_model, all_indexes, all_mappings)
        context = limit_context(retrieved_chunks)

        prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI IDCee. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, b·∫±ng ti·∫øng Vi·ªát v√† ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ ph·∫ßn Th√¥ng tin n·ªôi b·ªô.

üëâ N·∫øu kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p, tr·∫£ l·ªùi duy nh·∫•t c√¢u sau:
"T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu n·ªôi b·ªô ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

‚ùó Kh√¥ng ƒë∆∞·ª£c b·ªãa, suy ƒëo√°n ho·∫∑c th√™m n·ªôi dung ngo√†i d·ªØ li·ªáu cung c·∫•p. Ch·ªâ s·ª≠ d·ª•ng n·ªôi dung ch·ª©a t·ª´ kh√≥a: "{query}"

### C√¢u h·ªèi:
{query}

### Th√¥ng tin n·ªôi b·ªô:
{context}

### Tr·∫£ l·ªùi:
"""

        start = time.time()
        try:
            response = llm(prompt, max_new_tokens=192, stop=["###", "C√¢u h·ªèi:"])
            elapsed = time.time() - start

            if not response or str(response).strip() == "":
                print("ü§ñ IDCee: (Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi)")
            else:
                print(f"ü§ñ IDCee: {response.strip()}")
            print(f"‚è±Ô∏è Th·ªùi gian ph·∫£n h·ªìi: {elapsed:.2f} gi√¢y\n")

        except Exception as e:
            print(f"‚ùå L·ªói infer: {e}")
            continue

if __name__ == "__main__":
    main()
