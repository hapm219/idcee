import time
import json
import glob
from pathlib import Path

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

from load_model import load_llm

# ===== C·∫§U H√åNH ========
TOP_K = 3
INDEX_DIR = Path("data/index")
EMBEDDING_PATH = "data/encoder/bge-m3"

# ===== LOAD MODEL =====
def load_embedding_model():
    print("üì• ƒêang t·∫£i m√¥ h√¨nh embedding bge-m3...")
    return SentenceTransformer(EMBEDDING_PATH)

def load_all_indexes():
    all_indexes, all_mappings = [], []
    for faiss_path in glob.glob(str(INDEX_DIR / "**/index.faiss"), recursive=True):
        try:
            index = faiss.read_index(faiss_path)
            mapping_path = Path(faiss_path).parent / "mapping.json"
            if not mapping_path.exists():
                continue
            with open(mapping_path, encoding="utf-8") as f:
                mapping = json.load(f)
            all_indexes.append(index)
            all_mappings.append(mapping)
            print(f"‚úÖ Loaded index: {faiss_path}")
        except Exception as e:
            print(f"‚ùå L·ªói khi load {faiss_path}: {e}")
    return all_indexes, all_mappings

# ===== SEARCH & RETRIEVE =====
def search_similar_chunks(query, model, indexes, mappings, top_k=TOP_K):
    query_emb = model.encode(
        f"Represent this sentence for searching relevant passages: {query}",
        convert_to_numpy=True
    )
    results = []
    for index, texts in zip(indexes, mappings):
        D, I = index.search(np.array([query_emb]), top_k)
        results += [(score, texts[idx]) for score, idx in zip(D[0], I[0]) if 0 <= idx < len(texts)]
    return [text for _, text in sorted(results, key=lambda x: x[0])[:top_k]]

# ===== CONTEXT =====
def limit_context(chunks, max_chars=800):
    context = ""
    for c in chunks:
        if len(context) + len(c) > max_chars:
            break
        context += c + "\n\n"
    return context.strip()

# ===== MAIN LOOP =====
def main():
    llm = load_llm()
    embed_model = load_embedding_model()
    indexes, mappings = load_all_indexes()

    print("ü§ñ IDCee s·∫µn s√†ng! G√µ 'exit' ƒë·ªÉ tho√°t.\n")
    while True:
        user_input = input("üßë You: ").strip()
        if user_input.lower() in ["exit", "quit"]:
            break

        chunks = search_similar_chunks(user_input, embed_model, indexes, mappings)
        context = limit_context(chunks)

        prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI IDCee. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, b·∫±ng ti·∫øng Vi·ªát v√† ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ ph·∫ßn Th√¥ng tin n·ªôi b·ªô.

üëâ N·∫øu kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p, tr·∫£ l·ªùi duy nh·∫•t c√¢u sau:
"T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu n·ªôi b·ªô ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

‚ùó Kh√¥ng ƒë∆∞·ª£c b·ªãa, suy ƒëo√°n ho·∫∑c th√™m n·ªôi dung ngo√†i d·ªØ li·ªáu cung c·∫•p. Ch·ªâ s·ª≠ d·ª•ng n·ªôi dung ch·ª©a t·ª´ kh√≥a: "{user_input}"

### C√¢u h·ªèi:
{user_input}

### Th√¥ng tin n·ªôi b·ªô:
{context}

### Tr·∫£ l·ªùi:
"""

        start = time.time()
        try:
            result = llm(prompt, max_tokens=192)

            if isinstance(result, str):
                reply = result.strip()
            elif hasattr(result, "__iter__") and not isinstance(result, dict):
                reply = "".join(chunk for chunk in result).strip()
            elif isinstance(result, dict) and "choices" in result:
                reply = result["choices"][0]["text"].strip()
            else:
                reply = "(Kh√¥ng th·ªÉ x·ª≠ l√Ω ph·∫£n h·ªìi)"

            elapsed = time.time() - start
            if not reply:
                print("ü§ñ IDCee: (Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi)")
            else:
                print(f"ü§ñ IDCee: {reply}")
            print(f"‚è±Ô∏è Th·ªùi gian ph·∫£n h·ªìi: {elapsed:.2f} gi√¢y\n")

        except Exception as e:
            print(f"‚ùå L·ªói infer: {e}")

if __name__ == "__main__":
    main()
