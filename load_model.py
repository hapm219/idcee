import os
import time
import sys
import requests
import contextlib
import gc
from tqdm import tqdm
from llama_cpp import Llama  # ‚úÖ D√πng llama.cpp thay v√¨ ctransformers

MODEL_URL = "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
MODEL_PATH = "models/mistral-7b-instruct-v0.2.Q4_K_M.gguf"

def download_model():
    os.makedirs("models", exist_ok=True)
    if os.path.exists(MODEL_PATH):
        # print(f"üì¶ M√¥ h√¨nh ƒë√£ t·ªìn t·∫°i: {MODEL_PATH}")  # ·∫®n log n·∫øu c·∫ßn
        return

    print(f"‚¨áÔ∏è ƒêang t·∫£i m√¥ h√¨nh t·ª´ Hugging Face: {MODEL_URL}")
    with requests.get(MODEL_URL, stream=True) as r:
        r.raise_for_status()
        total_size = int(r.headers.get("Content-Length", 0))

        with open(MODEL_PATH, "wb") as f, tqdm(
            total=total_size, unit="B", unit_scale=True, desc="üì¶ T·∫£i v·ªÅ"
        ) as pbar:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
                pbar.update(len(chunk))
    print(f"‚úÖ ƒê√£ t·∫£i xong m√¥ h√¨nh: {MODEL_PATH}")

@contextlib.contextmanager
def suppress_output():
    with open(os.devnull, 'w') as fnull:
        old_stdout = sys.stdout
        old_stderr = sys.stderr
        sys.stdout = fnull
        sys.stderr = fnull
        try:
            yield
        finally:
            sys.stdout = old_stdout
            sys.stderr = old_stderr

def load_llm():
    download_model()
    print(f"\nüß† S·∫µn s√†ng t·∫°o m√¥ h√¨nh llama.cpp t·ª´: {MODEL_PATH}")

    def safe_infer(prompt, max_tokens=256, **kwargs):  # ‚úÖ H·ªó tr·ª£ stream & args kh√°c
        with suppress_output():
            llm = Llama(
                model_path=MODEL_PATH,
                n_ctx=1024,               # ‚úÖ Gi·ªõi h·∫°n context ƒë·ªÉ tr√°nh l·ªói Metal
                n_threads=os.cpu_count(),
                n_gpu_layers=32,         # ‚úÖ Khuy·∫øn ngh·ªã cho Mac M4
                use_mlock=True,
                verbose=False
            )
        result = llm(prompt, max_tokens=max_tokens, **kwargs)
        del llm
        gc.collect()
        return result

    return safe_infer
