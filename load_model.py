import os
import time
import requests
from tqdm import tqdm
from ctransformers import AutoModelForCausalLM

MODEL_URL = "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
MODEL_PATH = "models/mistral-7b-instruct-v0.2.Q4_K_M.gguf"

def download_model():
    os.makedirs("models", exist_ok=True)
    if not os.path.exists(MODEL_PATH):
        print(f"â¬‡ï¸ Äang táº£i mÃ´ hÃ¬nh tá»« Hugging Face: {MODEL_URL}")
        r = requests.get(MODEL_URL, stream=True)
        r.raise_for_status()
        total_size = int(r.headers.get("Content-Length", 0))

        with open(MODEL_PATH, "wb") as f, tqdm(
            total=total_size, unit="B", unit_scale=True, desc="ğŸ“¦ Táº£i vá»"
        ) as pbar:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
                pbar.update(len(chunk))

        print(f"âœ… ÄÃ£ táº£i xong mÃ´ hÃ¬nh: {MODEL_PATH}")
    else:
        print(f"ğŸ“¦ MÃ´ hÃ¬nh Ä‘Ã£ tá»“n táº¡i: {MODEL_PATH}")

def load_llm():
    download_model()
    print(f"\nğŸ§  Äang load mÃ´ hÃ¬nh GGUF tá»«: {MODEL_PATH}")
    start = time.time()

    llm = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        model_type="mistral",
        gpu_layers=40,               # âœ… Æ¯u tiÃªn dÃ¹ng CPU
        max_new_tokens=256,
        context_length=2048  # âœ… Ã©p sá»­ dá»¥ng Ä‘Ãºng context tá»« file
    )

    elapsed = time.time() - start
    device = "GPU" if getattr(llm.config, "gpu_layers", 0) > 0 else "CPU"
    context_len = getattr(llm, "context_length", "KhÃ´ng xÃ¡c Ä‘á»‹nh")

    print(f"âœ… ÄÃ£ load mÃ´ hÃ¬nh trÃªn {device} - Thá»i gian: {elapsed:.2f} giÃ¢y")
    print(f"ğŸ“ Context length há»— trá»£: {context_len} tokens")

    return llm
